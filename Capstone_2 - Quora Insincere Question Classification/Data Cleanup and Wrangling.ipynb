{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification \n",
    "## - Data Cleanup and Wrangling\n",
    "*******\n",
    "\n",
    "**The dataset of quora questions has been obtained from a kaggle competition, available at: https://www.kaggle.com/c/quora-insincere-questions-classification/data\n",
    "I will be using the training dataset since it also has the classification labels, which would make the classification problem a supervised machine learning problem. <br>\n",
    "The dataset contains about 1.3 million rows (questions), the question ID and the classification (sincere vs insincere) for each. The first step would be to get information from the question text column as that is the main part of the data available and that will be used to solve the classification problem. Then I will process the question text itself to make it more managable for the subsequent analysis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we will use the question text column to extract information which might be useful in classification. This includes:**\n",
    "* Length of the string\n",
    "* Number of capital letters\n",
    "* Ratio of number of capital letters to the length of the question\n",
    "* Number of words used\n",
    "* Number of unique words\n",
    "* Ratio of unique words to total number of words\n",
    "* Number of exclamation marks, question marks and other punctuations\n",
    "* Number of symbols / special characters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:01<00:00, 772297.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:08<00:00, 147037.65it/s]\n"
     ]
    }
   ],
   "source": [
    "data['length'] = data['question_text'].swifter.apply(lambda x : len(x))\n",
    "data['capitals'] = data['question_text'].swifter.apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['length']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:03<00:00, 417813.08it/s]\n",
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:05<00:00, 218827.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data['num_words'] = data['question_text'].swifter.apply(lambda comment: len(comment.split()))\n",
    "data['num_unique_words'] = data['question_text'].swifter.apply(lambda comment: len(set(w for w in comment.split())))\n",
    "data['words_vs_unique'] = data['num_unique_words'] / data['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_exclamation_marks'] = data['question_text'].apply(lambda comment: comment.count('!'))\n",
    "data['num_question_marks'] = data['question_text'].apply(lambda comment: comment.count('?'))\n",
    "data['num_punctuation'] = data['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "data['num_symbols'] = data['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  length  capitals  caps_vs_length  num_words  num_unique_words  \\\n",
       "0       0      72         2        0.027778         13                13   \n",
       "1       0      81         1        0.012346         16                15   \n",
       "2       0      67         2        0.029851         10                 8   \n",
       "3       0      57         4        0.070175          9                 9   \n",
       "4       0      77         3        0.038961         15                15   \n",
       "\n",
       "   words_vs_unique  num_exclamation_marks  num_question_marks  \\\n",
       "0           1.0000                      0                   1   \n",
       "1           0.9375                      0                   1   \n",
       "2           0.8000                      0                   2   \n",
       "3           1.0000                      0                   1   \n",
       "4           1.0000                      0                   1   \n",
       "\n",
       "   num_punctuation  num_symbols  \n",
       "0                0            0  \n",
       "1                1            0  \n",
       "2                0            0  \n",
       "3                0            0  \n",
       "4                0            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.187018e-02</td>\n",
       "      <td>7.067884e+01</td>\n",
       "      <td>2.860756e+00</td>\n",
       "      <td>4.557576e-02</td>\n",
       "      <td>1.280361e+01</td>\n",
       "      <td>1.213578e+01</td>\n",
       "      <td>9.666687e-01</td>\n",
       "      <td>1.742563e-03</td>\n",
       "      <td>1.057475e+00</td>\n",
       "      <td>2.904055e-01</td>\n",
       "      <td>1.543730e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.409197e-01</td>\n",
       "      <td>3.878428e+01</td>\n",
       "      <td>2.573325e+00</td>\n",
       "      <td>3.919927e-02</td>\n",
       "      <td>7.052437e+00</td>\n",
       "      <td>6.040779e+00</td>\n",
       "      <td>5.687022e-02</td>\n",
       "      <td>4.676167e-02</td>\n",
       "      <td>2.588720e-01</td>\n",
       "      <td>7.637911e-01</td>\n",
       "      <td>1.502499e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.523810e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>9.354839e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.389831e-02</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.500000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>5.882353e-02</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.017000e+03</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.340000e+02</td>\n",
       "      <td>9.600000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target        length      capitals  caps_vs_length     num_words  \\\n",
       "count  1.306122e+06  1.306122e+06  1.306122e+06    1.306122e+06  1.306122e+06   \n",
       "mean   6.187018e-02  7.067884e+01  2.860756e+00    4.557576e-02  1.280361e+01   \n",
       "std    2.409197e-01  3.878428e+01  2.573325e+00    3.919927e-02  7.052437e+00   \n",
       "min    0.000000e+00  1.000000e+00  0.000000e+00    0.000000e+00  1.000000e+00   \n",
       "25%    0.000000e+00  4.500000e+01  1.000000e+00    2.000000e-02  8.000000e+00   \n",
       "50%    0.000000e+00  6.000000e+01  2.000000e+00    3.389831e-02  1.100000e+01   \n",
       "75%    0.000000e+00  8.500000e+01  4.000000e+00    5.882353e-02  1.500000e+01   \n",
       "max    1.000000e+00  1.017000e+03  1.800000e+02    1.000000e+00  1.340000e+02   \n",
       "\n",
       "       num_unique_words  words_vs_unique  num_exclamation_marks  \\\n",
       "count      1.306122e+06     1.306122e+06           1.306122e+06   \n",
       "mean       1.213578e+01     9.666687e-01           1.742563e-03   \n",
       "std        6.040779e+00     5.687022e-02           4.676167e-02   \n",
       "min        1.000000e+00     9.523810e-02           0.000000e+00   \n",
       "25%        8.000000e+00     9.354839e-01           0.000000e+00   \n",
       "50%        1.100000e+01     1.000000e+00           0.000000e+00   \n",
       "75%        1.500000e+01     1.000000e+00           0.000000e+00   \n",
       "max        9.600000e+01     1.000000e+00           5.000000e+00   \n",
       "\n",
       "       num_question_marks  num_punctuation   num_symbols  \n",
       "count        1.306122e+06     1.306122e+06  1.306122e+06  \n",
       "mean         1.057475e+00     2.904055e-01  1.543730e-02  \n",
       "std          2.588720e-01     7.637911e-01  1.502499e-01  \n",
       "min          0.000000e+00     0.000000e+00  0.000000e+00  \n",
       "25%          1.000000e+00     0.000000e+00  0.000000e+00  \n",
       "50%          1.000000e+00     0.000000e+00  0.000000e+00  \n",
       "75%          1.000000e+00     0.000000e+00  0.000000e+00  \n",
       "max          1.000000e+01     3.500000e+01  2.000000e+01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have extracted useful information about the format and other characteristics of text, the next thing to do it normalize/process the text which can be used in eventual analysis. The steps of normalization include:**\n",
    "* Removing accented characters\n",
    "* Expanding contractions \n",
    "* Removing special characters\n",
    "* Lemmatizing the text to retain only the base words\n",
    "* Removing stop words which add unnecessary noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "import contractions\n",
    "import unicodedata\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:13<00:00, 93456.11it/s] \n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',text)    # remove extra newlines      \n",
    "    text = re.sub(' +', ' ', text)    # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "data['processed_text'] = data['question_text'].swifter.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:03<00:00, 373950.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "data['processed_text'] = data['processed_text'].swifter.apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [02:28<00:00, 8782.59it/s] \n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "data['processed_text'] = data['processed_text'].swifter.apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [00:05<00:00, 261146.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "data['processed_text'] = data['processed_text'].swifter.apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1306122/1306122 [01:31<00:00, 14345.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "data['processed_text'] = data['processed_text'].swifter.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "import swifter\n",
    "\n",
    "data['processed_text'] = data['processed_text'].swifter.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>quebec nationalist see province nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>adopt dog would encourage people adopt not shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>otto von guericke use magdeburg hemisphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>convert montra helicon mountain bike change tyre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  length  capitals  caps_vs_length  num_words  num_unique_words  \\\n",
       "0       0      72         2        0.027778         13                13   \n",
       "1       0      81         1        0.012346         16                15   \n",
       "2       0      67         2        0.029851         10                 8   \n",
       "3       0      57         4        0.070175          9                 9   \n",
       "4       0      77         3        0.038961         15                15   \n",
       "\n",
       "   words_vs_unique  num_exclamation_marks  num_question_marks  \\\n",
       "0           1.0000                      0                   1   \n",
       "1           0.9375                      0                   1   \n",
       "2           0.8000                      0                   2   \n",
       "3           1.0000                      0                   1   \n",
       "4           1.0000                      0                   1   \n",
       "\n",
       "   num_punctuation  num_symbols  \\\n",
       "0                0            0   \n",
       "1                1            0   \n",
       "2                0            0   \n",
       "3                0            0   \n",
       "4                0            0   \n",
       "\n",
       "                                      processed_text  \n",
       "0             quebec nationalist see province nation  \n",
       "1    adopt dog would encourage people adopt not shop  \n",
       "2  velocity affect time velocity affect space geo...  \n",
       "3         otto von guericke use magdeburg hemisphere  \n",
       "4   convert montra helicon mountain bike change tyre  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we have derived some meaningful information from the original dataset and also processed the question texts to make them suitable for further analysis.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
